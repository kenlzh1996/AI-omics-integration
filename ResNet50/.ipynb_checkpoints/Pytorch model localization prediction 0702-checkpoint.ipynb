{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df14cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pdb import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL #for images\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958310b8",
   "metadata": {},
   "source": [
    "### Values set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383b556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifications\n",
    "\n",
    "# Data splitting and path\n",
    "label_filepath = Path('SubCellBarcode.MCF7.0622.txt')\n",
    "image_folderpath = Path('images_patient')\n",
    "validation_fraction = 0.2\n",
    "\n",
    "# Model weight\n",
    "ModelWeightSpefication = 'random'\n",
    "\n",
    "if ModelWeightSpefication == 'random':\n",
    "    ModelWeights = None\n",
    "if ModelWeightSpefication == 'pretrained':\n",
    "    ModelWeights = models.squeezenet1_1.DEFAULT # it is the best available weight for resnet50 \n",
    "    \n",
    "# Optimizer\n",
    "Weight_Decay = 1e-4\n",
    "#  penalizes for too many weights - helps prevent overfitting\n",
    "\n",
    "Momentum = 0.9\n",
    "#  a way to smooth noise that is passed to the optimizer, 0.9\n",
    "#  momentum is deterimental without label smoothing\n",
    "\n",
    "Learner_rate = 1e-3\n",
    "\n",
    "# Loss Function\n",
    "Label_Smoothing = 0.3\n",
    "#  was 0.1\n",
    "#  sets the target of the loss function to something greater than 0 and less than 1\n",
    "#  helps prevent overfitting\n",
    "\n",
    "TransformOrNot = False\n",
    "\n",
    "ApplyClassWeightToLoss = False\n",
    "# Makes the loss weights equal to the fraction of each category label\n",
    "# Note, a layer is added to the model so the outputs of the model are equal to the number of categories\n",
    "#     This seems like a bug becuase that should already be the case\n",
    "\n",
    "# Run the model\n",
    "epochs = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026e403",
   "metadata": {},
   "source": [
    "### Device set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d24e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Is cuda available?: True\n",
      "cuda version: 11.7\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the GPU is available and store it as a variable so tensors can be moved to it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print('Is cuda available?:', torch.cuda.is_available())\n",
    "print('cuda version:', torch.version.cuda)\n",
    "dev = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e69732",
   "metadata": {},
   "source": [
    "### Transformation set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69482cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation\n",
    "# transforms = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15fa12b",
   "metadata": {},
   "source": [
    "### Custom image dataset set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9414395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CustomImageDataset class from the parent Dataset class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, input_df, img_dir, transform=None, target_transform=None, image_extension='.png'):\n",
    "        self.img_labels = input_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.image_extension = image_extension\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0] + self.image_extension)\n",
    "        image = read_image(img_path)\n",
    "        image = image.float()\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b6de2",
   "metadata": {},
   "source": [
    "### Training/Validation datasets set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b68d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to output training and validation annotation data frames from a single annotations file\n",
    "def GetTrainValAnnotDataFrames(labels, img_dir, val_frac):\n",
    "    # Inputs:\n",
    "    # labels: a directory to an comma separated file with image filenames (no extention, so gene name) and their labels\n",
    "    # img_dir: a directory to the folder containing the image files\n",
    "    # val_frac: the fraction of samples used for validation\n",
    "    \n",
    "    # read the csv file with the sample annotations\n",
    "    labels_df = pd.read_csv(labels, header=None, sep='\\t', dtype=str)\n",
    "    \n",
    "    # read the directory containing the images to get a list of filenames\n",
    "    #     strip the file extension so the filenames match the gene name in labels_df\n",
    "    image_list = [file for file in os.listdir(img_dir) if not file.startswith('.')] # take care of hidden files\n",
    "    gene_list = [os.path.splitext(file)[0] for file in image_list]  \n",
    "    \n",
    "    # remove entries in the labels_df that are not in the list of images\n",
    "    #     this is done so that the training and validation sets are divided correctly\n",
    "    common_indices = [item in gene_list for item in labels_df.iloc[:,0]]\n",
    "    labels_df = labels_df.loc[common_indices,:]\n",
    "    \n",
    "    # extract unique class label into an array\n",
    "    unique_labels = np.unique(labels_df.iloc[:,1])\n",
    "    \n",
    "    # split into training and validation set\n",
    "    train_df, valid_df = train_test_split(labels_df, test_size=val_frac, random_state=42)\n",
    "    \n",
    "    print(f\"Total labels: {len(labels_df)}\")\n",
    "    print(f\"Total genes in training are {len(train_df)}; in validation are {len(valid_df)}; sum is {len(train_df)+len(valid_df)}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    return train_df, valid_df, unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9deb7",
   "metadata": {},
   "source": [
    "### Show image and label to tensor set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46bb5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a torchvision tensor to the default format for matplotlib\n",
    "    # matplotlib.pyplot.imshow interprets an RGB vector as shape (N,M,3) \n",
    "    # torchvision.io.read_image reads an RGB image as shape (3,N,M)\n",
    "def ShowTorchvisionImage(ImageTensor):\n",
    "    ImageTensor_t1 = ImageTensor.transpose(1,2)\n",
    "    ImageTensor_t2 = ImageTensor_t1.transpose(0,2)\n",
    "    plt.imshow(ImageTensor_t2)\n",
    "    \n",
    "# Function to convert label numpy array to label tensor    \n",
    "def LabelArray_to_Tensor(input_array,labels_unique):\n",
    "    # converts a label array of strings to a tensor of floats\n",
    "    # Inputs:\n",
    "    #   input_array: the array of strings that are labels\n",
    "    #   labels_unique: an ordered list of the unique labels possible for the data set\n",
    "    # Outputs:\n",
    "    #   output_tensor: a tensor of floats, one unique float corresponding to each label\n",
    "    \n",
    "    int_label = np.arange(0,len(labels_unique))\n",
    "    i = 0\n",
    "    for label in labels_unique:\n",
    "        indices = [label == item for item in input_array]\n",
    "        input_array[indices] = int_label[i]\n",
    "        i = i+1\n",
    "\n",
    "    input_array = input_array.astype(float)\n",
    "    output_tensor = torch.as_tensor(input_array)\n",
    "    output_tensor = output_tensor.long()\n",
    "  \n",
    "    return(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109d1f6",
   "metadata": {},
   "source": [
    "### Training and testing epoch loop set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663d6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training the weights of the model\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "\n",
    "        X = X.to(torch.float32) # convert to float32 to avoid error stating byte expected but found float\n",
    "        X = X.to(dev)\n",
    "        pred = model(X) # predict classes using images from the training set\n",
    "        \n",
    "        # convert the labels into a tensor of intigers and compute the loss\n",
    "        y = np.asarray(y)\n",
    "        y = LabelArray_to_Tensor(y,unique_label_array)\n",
    "        y = y.to(dev)\n",
    "        loss = loss_fn(pred, y) # compute the loss based on model output and real labels\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # zero the parameter gradients \n",
    "        loss.backward() # backpropagate the loss\n",
    "        optimizer.step() # adjust parameters based on the calculated gradients\n",
    "             \n",
    "\n",
    "    loss = loss.item() # extract the loss value\n",
    "    \n",
    "    return(loss)\n",
    "            \n",
    "# Function for validating the weights of the model\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            \n",
    "            # convert to float32 to avoid error stating byte expected but found float\n",
    "            X = X.to(torch.float32)\n",
    "            X = X.to(dev)\n",
    "            pred = model(X)\n",
    "            \n",
    "            # convert the labels into a tensor of intigers and compute the loss\n",
    "            y = np.asarray(y)\n",
    "            y = LabelArray_to_Tensor(y,unique_label_array)\n",
    "            y = y.to(dev)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    return(test_loss,correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c2cb7",
   "metadata": {},
   "source": [
    "### Pretrained model set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d882c137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model (pretrained)\n",
    "\n",
    "# initial model weights\n",
    "# model = models.resnet50(weights=ModelWeights)\n",
    "# model = models.resnet50() # without initial weights defined - good for patients not re-arranged\n",
    "model = models.squeezenet1_1()\n",
    "\n",
    "# Update the out channels to match with number of class in our data\n",
    "if ApplyClassWeightToLoss:\n",
    "    model = models.squeezenet1_1()\n",
    "    model.classifier[1].out_channels = 4\n",
    "    \n",
    "#     for p in model.parameters(): # Freeze all parameters in the network\n",
    "#         p.requires_grad = False\n",
    "    \n",
    "    for p in model.classifier.parameters(): # Make classification layer trainable\n",
    "        p.requires_grad = False \n",
    "        \n",
    "    for p in model.features[-3:-1].parameters(): # retrain the last few layers\n",
    "        p.requires_grad = True\n",
    "    \n",
    "model = model.to(dev)\n",
    "\n",
    "# see the model architecture if desired\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d201c4",
   "metadata": {},
   "source": [
    "### Loss function and unbalanced class weight set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5baf7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "# This needs to be made general for the number of classes\n",
    "\n",
    "# funciton call\n",
    "# training, validation, unique_label_array = GetTrainValAnnotDataFrames(labels=label_filepath, img_dir=image_folderpath, val_frac=validation_fraction)\n",
    "\n",
    "# Wrap data into the custom pytorch dataset class\n",
    "# if TransformOrNot:\n",
    "#     TrainingData = CustomImageDataset(input_df=training,img_dir=image_folderpath, transform=transforms)\n",
    "#     ValidationData = CustomImageDataset(input_df=validation,img_dir=image_folderpath, transform=transforms)\n",
    "# else:\n",
    "#     TrainingData = CustomImageDataset(input_df=training,img_dir=image_folderpath)\n",
    "#     ValidationData = CustomImageDataset(input_df=validation,img_dir=image_folderpath)\n",
    "    \n",
    "# loss function\n",
    "\n",
    "if ApplyClassWeightToLoss:\n",
    "    # Nate's weight tensor([ 2.2273, 13.4550,  3.5913,  5.0439]\n",
    "    # Weights for each class for loss function\n",
    "#     n_cytosol = float(sum(TrainingData.img_labels.iloc[:,1]=='Cytosol'))\n",
    "#     n_mitochondria = float(sum(TrainingData.img_labels.iloc[:,1]=='Mitochondria'))\n",
    "#     n_nuclear = float(sum(TrainingData.img_labels.iloc[:,1]=='Nuclear'))\n",
    "#     n_secretory = float(sum(TrainingData.img_labels.iloc[:,1]=='Secretory'))\n",
    "#     n_total = len(TrainingData.img_labels.iloc[:,1])\n",
    "    \n",
    "    # zhuoheng's weight\n",
    "    # train_weight = compute_sample_weight('balanced', y=train_labels)\n",
    "    LossWeights = torch.tensor([0.59259259,5.33333333,0.84210526,1.06666667]).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=Label_Smoothing, weight=LossWeights)\n",
    "\n",
    "else:\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=Label_Smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cdeb0",
   "metadata": {},
   "source": [
    "### Optimizer set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3af4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "def Optimizer(name, model):\n",
    "    if name == 'SGD_weight':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=Learner_rate, weight_decay=Weight_Decay, momentum=Momentum)\n",
    "    if name == 'SGD_default':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=Learner_rate)\n",
    "    if name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=Learner_rate)\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bdc13",
   "metadata": {},
   "source": [
    "### Early stopping set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4869dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = torch.optim.lr_scheduler.ReduceLROnPlateau(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4761e",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f476c7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labels: 6173\n",
      "Total genes in training are 4938; in validation are 1235; sum is 6173\n",
      "Unique labels: ['Cytosol' 'Mitochondria' 'Nuclear' 'Secretory']\n"
     ]
    }
   ],
   "source": [
    "# funciton call\n",
    "training, validation, unique_label_array = GetTrainValAnnotDataFrames(labels=label_filepath, img_dir=image_folderpath, val_frac=validation_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58bd97f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Cytosol\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wrap data into the custom pytorch dataset class\n",
    "if TransformOrNot:\n",
    "    TrainingData = CustomImageDataset(input_df=training,img_dir=image_folderpath, transform=transforms)\n",
    "    ValidationData = CustomImageDataset(input_df=validation,img_dir=image_folderpath, transform=transforms)\n",
    "else:\n",
    "    TrainingData = CustomImageDataset(input_df=training,img_dir=image_folderpath)\n",
    "    ValidationData = CustomImageDataset(input_df=validation,img_dir=image_folderpath)\n",
    "\n",
    "# Define Data Loaders\n",
    "train_dataloader = DataLoader(TrainingData, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(ValidationData, batch_size=64, shuffle=True)\n",
    "\n",
    "# Display a training image and its corresponding label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "img = train_features[0].squeeze() #no squeeze\n",
    "label = train_labels[0]\n",
    "ShowTorchvisionImage(img)\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   Train Loss   Val Loss   Train Accuracy   Val Accuracy\n",
      "\n",
      "    0     3.696576   3.546085       44.896719%     47.611336%\n",
      "    1     3.626653   3.540665       44.896719%     47.449393%\n",
      "    2     3.579425   3.535417       44.896719%     47.449393%\n",
      "    3     3.458049   3.571214       44.896719%     47.449393%\n",
      "    4     3.517411   3.552947       44.896719%     47.449393%\n",
      "    5     3.455444   3.539004       44.896719%     47.449393%\n",
      "    6     3.460419   3.535198       44.896719%     47.449393%\n",
      "    7     3.578037   3.533363       44.896719%     47.449393%\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "# set up performance dataframe to record loss and accuracy of each epoch\n",
    "PerformanceDF = pd.DataFrame(index=range(epochs),columns=['Epoch','TrainLoss','ValLoss','TrainAcc','ValAcc'])\n",
    "print(f\"{'Epoch':>5s}{'Train Loss':>13s}{'Val Loss':>11s}{'Train Accuracy':>17s}{'Val Accuracy':>15s}\\n\")\n",
    "\n",
    "# call optimizer\n",
    "optimizer = Optimizer('Adam', model)\n",
    "    \n",
    "for t in range(epochs): \n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    val_loss, val_accuracy = test_loop(val_dataloader, model, loss_fn)\n",
    "    train_loss2, train_accuracy = test_loop(train_dataloader, model, loss_fn)\n",
    "    Epoch = t\n",
    "    print(f\"{str(Epoch):>5s}{train_loss:>13f}{val_loss:>11f}{train_accuracy*100:>16f}%{val_accuracy*100:>14f}%\")\n",
    "    PerformanceDF.loc[t,:]=[Epoch,train_loss,val_loss,train_accuracy*100,val_accuracy*100]\n",
    "    \n",
    "    # note: I am not sure how to load the MSD into the model\n",
    "    if Epoch == 0:\n",
    "        highest_val_accuracy = 0 \n",
    "        MSD0 = copy.deepcopy(model.state_dict())\n",
    "    if val_accuracy > highest_val_accuracy:\n",
    "        highest_val_accuracy = val_accuracy\n",
    "        MSD_Best = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ec48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best recorded epoch validation accuracy: ' + str(round(100*highest_val_accuracy,2)))\n",
    "\n",
    "BestModel = copy.deepcopy(model)\n",
    "BestModel.load_state_dict(MSD_Best)\n",
    "val_loss,best_val_accuracy = test_loop(val_dataloader, BestModel, loss_fn)\n",
    "print('Best epoch recalculated validation accuracy: ' + str(round(100*best_val_accuracy,2)))\n",
    "\n",
    "FirstModel = copy.deepcopy(model)\n",
    "FirstModel.load_state_dict(MSD0)\n",
    "val_loss,first_val_accuracy = test_loop(val_dataloader, FirstModel, loss_fn)\n",
    "print('First epoch validation Accuracy: ' + str(round(100*first_val_accuracy,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build confusion matrix\n",
    "#   https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sn\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for X, y in val_dataloader:\n",
    "    \n",
    "        X = X.to(torch.float32)\n",
    "        X = X.to(dev)\n",
    "        output = model(X) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        output_string = [unique_label_array[i] for i in output]\n",
    "        \n",
    "        y_pred.extend(output_string) # Save Prediction\n",
    "        \n",
    "        labels = y\n",
    "        y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "# classes = unique_labels\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix, display_labels=unique_label_array)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "# cf_matrix_norm = np.zeros((len(classes),len(classes)))\n",
    "# for i in np.arange(0,len(classes)):\n",
    "#     cf_matrix_norm[i,:] = cf_matrix[i,:]/sum(cf_matrix[i,:])\n",
    "\n",
    "# df_cm = pd.DataFrame(cf_matrix_norm, index = [i for i in classes],\n",
    "#                      columns = [i for i in classes])\n",
    "# plt.figure(figsize = (6,5))\n",
    "# hm = sn.heatmap(df_cm, annot=True)\n",
    "# hm.set(xlabel='\\n\\nPrediction', ylabel='Label\\n\\n')\n",
    "# plt.savefig('output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph accuracy for training and validation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(PerformanceDF['Epoch'], PerformanceDF['TrainAcc'])\n",
    "plt.plot(PerformanceDF['Epoch'], PerformanceDF['ValAcc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Orientation of Confusion Matrix\n",
    "#     Data labels go down the rows and predictions label the columns\n",
    "\n",
    "# Print Confusion Matrix Row Sums\n",
    "ConfusionMatrix_RowSums = [sum(cf_matrix[i,:]) for i in np.arange(0,len(unique_label_array))]\n",
    "print(ConfusionMatrix_RowSums)\n",
    "\n",
    "# Print Quantity of Items with each label\n",
    "ValDF_UniqueLabelSums = [sum(validation.iloc[:,1]==unique_label_array[i]) for i in np.arange(0,len(unique_label_array))]\n",
    "print(ValDF_UniqueLabelSums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed69ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbdcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c45c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
